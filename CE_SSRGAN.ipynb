{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "SS6PkVwKYuDI",
        "SS6_7rlqxXAY",
        "cMuQ9lFZTB8q",
        "kio-qPj52WNt",
        "Rsa7c7MN2YDR",
        "qi3axAio2Zqx",
        "MrvVnvwq2bX7",
        "rBTkFSidlC2D",
        "0gpUqFL6sz6L"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Download dataset\n"
      ],
      "metadata": {
        "id": "di2RJo20w1Ug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hk_E06FGD-4Y",
        "outputId": "51f181c8-64f3-4f01-ff32-3b07bfbc1efd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 17FfAoAsvEhTFaQXB97xoVVKN4rDeBE41\n",
        "!gdown 1b4Z_ngrkzTxZ1U1bWqCfE9gERvRZCYqq\n",
        "\n",
        "!unzip '/content/Train_144.mat.zip'\n",
        "!unzip '/content/Test_144.mat.zip'\n",
        "!mkdir 'test'\n",
        "!mkdir 'train'\n",
        "!mv '/content/Test_144.mat' '/content/test/Test_144.mat'\n",
        "!mv '/content/Train_144.mat' '/content/train/Train_144.mat'"
      ],
      "metadata": {
        "id": "XiY4-Nq9C4un",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0bdb504-ea04-4845-c395-1f68c790a613"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=17FfAoAsvEhTFaQXB97xoVVKN4rDeBE41\n",
            "To: /content/Train_144.mat.zip\n",
            "100% 2.48G/2.48G [00:35<00:00, 69.8MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1b4Z_ngrkzTxZ1U1bWqCfE9gERvRZCYqq\n",
            "To: /content/Test_144.mat.zip\n",
            "100% 355M/355M [00:05<00:00, 59.9MB/s]\n",
            "Archive:  /content/Train_144.mat.zip\n",
            "  inflating: Train_144.mat           \n",
            "Archive:  /content/Test_144.mat.zip\n",
            "  inflating: Test_144.mat            \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1Vf4pJJSU8uUJOVnBqQGogB0Fi9Ll_Yt3\n",
        "!gdown 1RQ8fj1UfUzroI1g_jmgTMbT3W5teSMZB\n",
        "\n",
        "!unzip '/content/Train_16_144.mat.zip'\n",
        "!unzip '/content/mmWave_Test.mat.zip'\n",
        "!mkdir 'test'\n",
        "!mkdir 'train'\n",
        "!mv '/content/mmWave_Test.mat' '/content/test/Test_16_144.mat'\n",
        "!mv '/content/Train_16_144.mat' '/content/train/Train_16_144.mat'"
      ],
      "metadata": {
        "id": "Uh7AY3b9oHWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprations"
      ],
      "metadata": {
        "id": "0jmMqg_vYq5b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7LO-5MvQzMl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "442db373-17ee-4d3f-cd18-a518ac6c298b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mat73\n",
            "  Downloading mat73-0.61-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from mat73) (3.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mat73) (1.23.5)\n",
            "Installing collected packages: mat73\n",
            "Successfully installed mat73-0.61\n"
          ]
        }
      ],
      "source": [
        "!pip install mat73\n",
        "import torch\n",
        "from tabulate import tabulate\n",
        "from scipy.io import savemat\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import time\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import sys\n",
        "import random\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy.linalg as LA\n",
        "import os\n",
        "import math\n",
        "import mat73\n",
        "from scipy.interpolate import Rbf\n",
        "import scipy.io as sio\n",
        "from numpy import *\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-Processing"
      ],
      "metadata": {
        "id": "yO1Mz51xCw9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DFT matrix\n",
        "def DFT_matrix(N):\n",
        "    m, n = np.meshgrid(np.arange(N), np.arange(N))\n",
        "    omega = np.exp( - 2 * np.pi * 1j / N )\n",
        "    D = np.power( omega, m * n )\n",
        "    return D\n",
        "\n",
        "\n",
        "# Create a function to interpolate using Rbf\n",
        "def interpolate(matrix):\n",
        "    rows, cols = np.nonzero(matrix)\n",
        "    values = matrix[rows, cols]\n",
        "    grid_x, grid_y = np.meshgrid(np.arange(matrix.shape[1]), np.arange(matrix.shape[0]))\n",
        "    rbf = Rbf(cols, rows, values, function='gaussian')\n",
        "    interpolated_matrix = rbf(grid_x, grid_y)\n",
        "    return interpolated_matrix"
      ],
      "metadata": {
        "id": "YLtAnVlkC93N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "M_t = 36\n",
        "Nt = 16\n",
        "M = 144\n",
        "ch = 3\n",
        "\n",
        "snr = 20\n",
        "SNR=10.0**(snr/10.0) # transmit power\n",
        "\n",
        "IRS_index = range(0,M,4)\n",
        "\n",
        "PHI = np.zeros((M,M_t), dtype=complex)\n",
        "PHI_t = DFT_matrix(len(IRS_index))\n",
        "\n",
        "for c in range(M_t):\n",
        "  index = IRS_index[c]\n",
        "  for i in range(M_t):\n",
        "    PHI[index,i]=PHI_t[c,i]\n",
        "print(M_t)\n",
        "print(len(IRS_index))\n",
        "\n",
        "phi = np.dot(PHI,np.transpose(np.conjugate(PHI)))"
      ],
      "metadata": {
        "id": "yW-cMVViDMVF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d91d493d-6d73-458f-e6c4-03bbbdeee0ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "36\n",
            "36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############## training set generation ##################\n",
        "file_dir = '/content/train'\n",
        "\n",
        "data_num_train = 40000\n",
        "data_num_file = 40000\n",
        "Y=np.zeros((Nt,M_t), dtype=float)\n",
        "H_train=np.zeros((data_num_train,Nt,M,ch), dtype=float)\n",
        "H_train_noisy=np.zeros((data_num_train,Nt,M,ch), dtype=float)\n",
        "filedir = os.listdir(file_dir)  # type the path of training data\n",
        "SNR_factor = 31.1358\n",
        "SNRr = 0\n",
        "\n",
        "for filename in filedir:\n",
        "    newname = os.path.join(file_dir, filename)\n",
        "    data = mat73.loadmat(newname)\n",
        "    channel = data['H']\n",
        "    # data = []\n",
        "    for i in range(data_num_file):\n",
        "        H=channel[:,:,i]\n",
        "        H_re = np.real(H)\n",
        "        H_im = np.imag(H)\n",
        "        H_ab = np.abs(H)\n",
        "        H_train[i, :, :, 0] = H_re\n",
        "        H_train[i, :, :, 1] = H_im\n",
        "        H_train[i, :, :, 2] = H_ab\n",
        "\n",
        "        N = np.random.normal(0, 1 / np.sqrt(2), size=(Nt, M)) + 1j * np.random.normal(0, 1 / np.sqrt(2), size=(Nt, M))\n",
        "        N_hat = np.dot(N,phi)\n",
        "\n",
        "        H_hat = np.dot(H,phi)\n",
        "        Y = H_hat + 1.0 / np.sqrt(SNR_factor*SNR) * N_hat\n",
        "        H_hat_re = np.real(Y)\n",
        "        H_hat_im = np.imag(Y)\n",
        "        H_hat_ab = np.abs(Y)\n",
        "        SNRr = SNRr + SNR_factor*SNR * (LA.norm(H_hat)) ** 2 / (LA.norm(N_hat)) ** 2\n",
        "\n",
        "        H_train_noisy[i, :, :, 0] = H_hat_re\n",
        "        H_train_noisy[i, :, :, 1] = H_hat_im\n",
        "        H_train_noisy[i, :, :, 2] = H_hat_ab\n",
        "\n",
        "print(SNRr/(data_num_train))\n",
        "SNRR = 10 * np.log10(SNRr/(data_num_train))\n",
        "print(SNRR)\n",
        "print(H_train.shape,H_train_noisy.shape)\n",
        "# channel = []"
      ],
      "metadata": {
        "id": "xeuVSUb1C4Up",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcfc4b37-6bca-4c05-c182-49d26f084e2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100.19660318333281\n",
            "20.00852998539806\n",
            "(40000, 16, 144, 3) (40000, 16, 144, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset and prepration"
      ],
      "metadata": {
        "id": "SS6PkVwKYuDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class mydataset(Dataset):\n",
        "  def __init__(self , datas , labels):\n",
        "    super(mydataset,self).__init__()\n",
        "    self.datas = torch.tensor(datas , dtype=torch.float)\n",
        "    self.datas = torch.permute(self.datas, (0,3,1,2)) # must to use torch.permute beacuse the channel must be after batch size [batch_size , channel , nr , nt]\n",
        "\n",
        "    self.labels = torch.tensor(labels , dtype=torch.float)\n",
        "    self.labels = torch.permute(self.labels, (0,3,1,2)) #  must to use torch.permute beacuse the channel must be after batch size [batch_size , channel , nr , nt]\n",
        "  def __getitem__(self, index):\n",
        "    return self.datas[index] , self.labels[index]\n",
        "  def __len__(self):\n",
        "    return self.datas.shape[0]"
      ],
      "metadata": {
        "id": "9A88laDXYn7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "trainset = mydataset(H_train_noisy , H_train)\n",
        "# trainset,valset = train_test_split(trainset,test_size=0.2,shuffle=True,random_state=123)\n",
        "\n",
        "trainloader = DataLoader(trainset , batch_size = batch_size , shuffle = True, drop_last=True)\n",
        "# valloader = DataLoader(valset , batch_size = batch_size , shuffle = True, drop_last=True)"
      ],
      "metadata": {
        "id": "O79mUFs1Ypvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_test=10\n",
        "trainset2 = mydataset(H_train_noisy , H_train)\n",
        "trainloader2 = DataLoader(trainset2 , batch_size = batch_test , shuffle = True, drop_last=True)\n",
        "\n",
        "def train(testloader,model,size):\n",
        "  nmse=np.zeros((size,1), dtype=float16)\n",
        "  k = 0\n",
        "\n",
        "  model.eval()\n",
        "  for (input,label) in tqdm(testloader):\n",
        "    nmse2=torch.zeros((batch_test,1), dtype=torch.float32)\n",
        "\n",
        "    input,label = input.to(device),label.to(device)\n",
        "    decoded_channel = model(input)\n",
        "\n",
        "    for n in range(batch_test):\n",
        "        MSE=((label[n,:,:,:]-decoded_channel[n,:,:,:])**2).sum()\n",
        "        norm_real=((label[n,:,:,:])**2).sum()\n",
        "        nmse2[n]=MSE/norm_real\n",
        "\n",
        "    a = nmse2.sum()\n",
        "    nmse[k] = a.detach().numpy()\n",
        "    k = k + 1\n",
        "\n",
        "  return nmse"
      ],
      "metadata": {
        "id": "7rrD_N49ae4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "H_train_noisy =[]\n",
        "H_train = []\n",
        "# data = []\n",
        "# channel = []\n",
        "# x=trainset.__getitem__(3)[0]\n",
        "# type(x)"
      ],
      "metadata": {
        "id": "_0LZRb2Se4qh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NMSE Valid"
      ],
      "metadata": {
        "id": "SS6_7rlqxXAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "############## training set generation ##################\n",
        "file_dir = '/content/train'\n",
        "\n",
        "data_num_train = 10000\n",
        "data_num_file = 10000\n",
        "Y=np.zeros((Nt,M_t), dtype=float)\n",
        "H_valid=np.zeros((data_num_train,Nt,M,ch), dtype=float)\n",
        "H_valid_noisy=np.zeros((data_num_train,Nt,M,ch), dtype=float)\n",
        "filedir = os.listdir(file_dir)  # type the path of training data\n",
        "SNR_factor = 31.1358\n",
        "SNRr = 0\n",
        "\n",
        "for filename in filedir:\n",
        "    newname = os.path.join(file_dir, filename)\n",
        "    # data = mat73.loadmat(newname)\n",
        "    channel = data['H']\n",
        "    # data = []\n",
        "    for i in range(40000,50000):\n",
        "        H=channel[:,:,i]\n",
        "        H_re = np.real(H)\n",
        "        H_im = np.imag(H)\n",
        "        H_ab = np.abs(H)\n",
        "        H_valid[i-40000, :, :, 0] = H_re\n",
        "        H_valid[i-40000, :, :, 1] = H_im\n",
        "        H_valid[i-40000, :, :, 2] = H_ab\n",
        "\n",
        "        N = np.random.normal(0, 1 / np.sqrt(2), size=(Nt, M)) + 1j * np.random.normal(0, 1 / np.sqrt(2), size=(Nt, M))\n",
        "        N_hat = np.dot(N,phi)\n",
        "\n",
        "        H_hat = np.dot(H,phi)\n",
        "        Y = H_hat + 1.0 / np.sqrt(SNR_factor*SNR) * N_hat\n",
        "        H_hat_re = np.real(Y)\n",
        "        H_hat_im = np.imag(Y)\n",
        "        H_hat_ab = np.abs(Y)\n",
        "        SNRr = SNRr + SNR_factor*SNR * (LA.norm(H_hat)) ** 2 / (LA.norm(N_hat)) ** 2\n",
        "\n",
        "        H_valid_noisy[i-40000, :, :, 0] = H_hat_re\n",
        "        H_valid_noisy[i-40000, :, :, 1] = H_hat_im\n",
        "        H_valid_noisy[i-40000, :, :, 2] = H_hat_ab\n",
        "\n",
        "print(SNRr/(data_num_train))\n",
        "SNRR = 10 * np.log10(SNRr/(data_num_train))\n",
        "print(SNRR)\n",
        "print(H_valid.shape,H_valid_noisy.shape)\n",
        "channel = []"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RonJgfZDORHC",
        "outputId": "2042577a-aae5-4d98-fe58-2301b3a0f585"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100.58000745257286\n",
            "20.025116634643787\n",
            "(10000, 16, 144, 3) (10000, 16, 144, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_test=10\n",
        "validset = mydataset(H_valid_noisy , H_valid)\n",
        "validloader = DataLoader(validset , batch_size = batch_test , shuffle = True, drop_last=True)\n",
        "\n",
        "def valid(testloader,model,size):\n",
        "  nmse=np.zeros((size,1), dtype=float16)\n",
        "  k = 0\n",
        "\n",
        "  model.eval()\n",
        "  for (input,label) in tqdm(testloader):\n",
        "    nmse2=torch.zeros((batch_test,1), dtype=torch.float32)\n",
        "\n",
        "    input,label = input.to(device),label.to(device)\n",
        "    t1=time.time()\n",
        "    decoded_channel = model(input)\n",
        "\n",
        "    t2=time.time()\n",
        "\n",
        "    for n in range(batch_test):\n",
        "        MSE=((label[n,:,:,:]-decoded_channel[n,:,:,:])**2).sum()\n",
        "        norm_real=((label[n,:,:,:])**2).sum()\n",
        "        nmse2[n]=MSE/norm_real\n",
        "\n",
        "    a = nmse2.sum()\n",
        "    nmse[k] = a.detach().numpy()\n",
        "    k = k + 1\n",
        "\n",
        "  return nmse"
      ],
      "metadata": {
        "id": "e1oJ2J_Dbu0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "H_valid = []\n",
        "H_valid_noisy = []\n",
        "data = []\n",
        "channel = []\n",
        "# x = trainset.__getitem__(0)"
      ],
      "metadata": {
        "id": "t61OwSs6cGnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SSRGAN"
      ],
      "metadata": {
        "id": "cMuQ9lFZTB8q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ConvBlock"
      ],
      "metadata": {
        "id": "kio-qPj52WNt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            discriminator=False,\n",
        "            use_act=True,\n",
        "            use_bn=True,\n",
        "            **kwargs,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.use_act = use_act\n",
        "        self.cnn = nn.Conv2d(in_channels, out_channels, **kwargs, bias=not use_bn)\n",
        "        self.bn = nn.BatchNorm2d(out_channels) if use_bn else nn.Identity()\n",
        "        self.act = (\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "            if discriminator\n",
        "            else nn.PReLU(num_parameters=out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.act(self.bn(self.cnn(x))) if self.use_act else self.bn(self.cnn(x))\n",
        "\n"
      ],
      "metadata": {
        "id": "SUmFBRVgTGJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UpsampleBlock"
      ],
      "metadata": {
        "id": "Rsa7c7MN2YDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UpsampleBlock(nn.Module):\n",
        "    def __init__(self, in_c, scale_factor):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_c, in_c * scale_factor ** 2, 3, 1, 1)\n",
        "        self.ps = nn.PixelShuffle(scale_factor)  # in_c * 4, H, W --> in_c, H*2, W*2\n",
        "        self.act = nn.PReLU(num_parameters=in_c)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.act(self.ps(self.conv(x)))"
      ],
      "metadata": {
        "id": "M1WSXzatnHvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ResidualBlock"
      ],
      "metadata": {
        "id": "qi3axAio2Zqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "        self.block1 = ConvBlock(\n",
        "            in_channels,\n",
        "            in_channels,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            padding=1\n",
        "        )\n",
        "        self.block2 = ConvBlock(\n",
        "            in_channels,\n",
        "            in_channels,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            padding=1,\n",
        "            use_act=False,\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.block1(x)\n",
        "        out = self.block2(out)\n",
        "        return out + x\n"
      ],
      "metadata": {
        "id": "e7qUK24QnHl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generator & Discriminator"
      ],
      "metadata": {
        "id": "MrvVnvwq2bX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "'''''''''''''''''''''''''''''''''''''''''''Generator'''''''''''''''''''''''''''''''''''''''''''\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, in_channels=ch, num_channels=64, num_blocks = 2):\n",
        "        super().__init__()\n",
        "        self.initial = ConvBlock(in_channels, num_channels, kernel_size=9, stride=1, padding=4, use_bn=False)\n",
        "        self.residuals = nn.Sequential(*[ResidualBlock(num_channels) for _ in range(num_blocks)])\n",
        "        self.convblock = ConvBlock(num_channels, num_channels, kernel_size=3, stride=1, padding=1, use_act=False)\n",
        "        self.upsamples = nn.Sequential(UpsampleBlock(num_channels, 1), UpsampleBlock(num_channels, 1))\n",
        "        self.final = nn.Conv2d(num_channels, in_channels, kernel_size=9,  padding='same')\n",
        "\n",
        "    def forward(self, x):\n",
        "        initial = self.initial(x)\n",
        "        x = self.residuals(initial)\n",
        "        x = self.convblock(x) + initial\n",
        "        x = self.upsamples(x)\n",
        "        return torch.tanh(self.final(x))\n",
        "\n",
        "'''''''''''''''''''''''''''''''''''''''''Discriminator'''''''''''''''''''''''''''''''''''''''''\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, in_channels=ch, features=[64, 128, 256, 512]):\n",
        "        super().__init__()\n",
        "        blocks = []\n",
        "        for idx, feature in enumerate(features):\n",
        "            blocks.append(\n",
        "                ConvBlock(\n",
        "                    in_channels,\n",
        "                    feature,\n",
        "                    kernel_size=3,\n",
        "                    stride=1 + idx % 2,\n",
        "                    padding=1,\n",
        "                    discriminator=True,\n",
        "                    use_act=True,\n",
        "                    use_bn=False if idx == 0 else True,\n",
        "                )\n",
        "            )\n",
        "            in_channels = feature\n",
        "\n",
        "        self.blocks = nn.Sequential(*blocks)\n",
        "\n",
        "        self.conv = nn.Sequential(nn.Conv2d(in_channels=512,out_channels=32,kernel_size=1),\n",
        "                                  nn.ReLU(),\n",
        "                                  nn.ConvTranspose2d(in_channels=32, out_channels=ch, kernel_size=4,stride=4),\n",
        "                                  )\n",
        "\n",
        "        self.linear = nn.Sequential(\n",
        "\n",
        "            nn.Flatten()\n",
        "\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.blocks(x)\n",
        "        x = self.conv(x)\n",
        "        x = self.linear(x)\n",
        "\n",
        "        x = torch.reshape(x, (batch_size, ch, Nt, M))\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "bm4Ldy6RnHis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen = Generator(in_channels=ch).to(device)\n",
        "disc = Discriminator(in_channels=ch).to(device)\n",
        "gen"
      ],
      "metadata": {
        "id": "HdmHZ73vV-34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff4db4a3-b9d7-4431-f57f-8a8c2da655c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Generator(\n",
              "  (initial): ConvBlock(\n",
              "    (cnn): Conv2d(3, 64, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n",
              "    (bn): Identity()\n",
              "    (act): PReLU(num_parameters=64)\n",
              "  )\n",
              "  (residuals): Sequential(\n",
              "    (0): ResidualBlock(\n",
              "      (block1): ConvBlock(\n",
              "        (cnn): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act): PReLU(num_parameters=64)\n",
              "      )\n",
              "      (block2): ConvBlock(\n",
              "        (cnn): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act): PReLU(num_parameters=64)\n",
              "      )\n",
              "    )\n",
              "    (1): ResidualBlock(\n",
              "      (block1): ConvBlock(\n",
              "        (cnn): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act): PReLU(num_parameters=64)\n",
              "      )\n",
              "      (block2): ConvBlock(\n",
              "        (cnn): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act): PReLU(num_parameters=64)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (convblock): ConvBlock(\n",
              "    (cnn): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (act): PReLU(num_parameters=64)\n",
              "  )\n",
              "  (upsamples): Sequential(\n",
              "    (0): UpsampleBlock(\n",
              "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (ps): PixelShuffle(upscale_factor=1)\n",
              "      (act): PReLU(num_parameters=64)\n",
              "    )\n",
              "    (1): UpsampleBlock(\n",
              "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (ps): PixelShuffle(upscale_factor=1)\n",
              "      (act): PReLU(num_parameters=64)\n",
              "    )\n",
              "  )\n",
              "  (final): Conv2d(64, 3, kernel_size=(9, 9), stride=(1, 1), padding=same)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(64, ch, 16, 144).to(device)\n",
        "s = gen(x).shape\n",
        "print(s)"
      ],
      "metadata": {
        "id": "x5vB7RaGt3u3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4531bcce-4f7c-4df9-e437-5dfb42c3cf30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 3, 16, 144])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The gen has {count_parameters(gen):,} trainable parameters')\n",
        "print(f'The disc has {count_parameters(disc):,} trainable parameters')\n"
      ],
      "metadata": {
        "id": "ABoGQtKTuyri",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a4f4fdd-44b8-45c8-83b2-8527e6758ba4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The gen has 290,499 trainable parameters\n",
            "The disc has 1,569,827 trainable parameters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-b70f27d26e37>:2: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
            "  return sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Default setting"
      ],
      "metadata": {
        "id": "fSvsih7lY5J4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LEARNING_RATE=0.0002\n",
        "opt_gen = torch.optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.999))\n",
        "opt_disc = torch.optim.Adam(disc.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.999))\n",
        "\n",
        "lr_scheduler_gen = torch.optim.lr_scheduler.StepLR(optimizer = opt_gen , step_size = 20 , gamma=0.6)\n",
        "lr_scheduler_disc = torch.optim.lr_scheduler.StepLR(optimizer = opt_disc , step_size = 20 , gamma=0.6)\n",
        "\n",
        "mse = nn.MSELoss()\n",
        "criterion = nn.MSELoss()"
      ],
      "metadata": {
        "id": "nV0lh4tkTJ5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "  this class is for calculating the final loss of each epoch\n",
        "'''\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "metadata": {
        "id": "RFVOVPzmmtX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "logging.getLogger().setLevel(logging.INFO)\n",
        "logger = logging.getLogger('SRGAN: ')"
      ],
      "metadata": {
        "id": "oLrcCA7jniUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "VvUVukxsZBIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    checkpoint = {\n",
        "        \"state_dict\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "    }\n",
        "    torch.save(checkpoint, filename)\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint_file, model, optimizer, lr):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    checkpoint = torch.load(checkpoint_file, map_location=device)\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "\n",
        "    # # If we don't do this then it will just have learning rate of old checkpoint\n",
        "    # # and it will lead to many hours of debugging \\:\n",
        "    # for param_group in optimizer.param_groups:\n",
        "    #     param_group[\"lr\"] = lr"
      ],
      "metadata": {
        "id": "VlL0e6Hu7daF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "discloss = []\n",
        "genloss = []\n",
        "def train_fn(loader, disc, gen, opt_gen, opt_disc, mse):\n",
        "    gen.train()\n",
        "    disc.train()\n",
        "\n",
        "    loop = tqdm(loader, leave=True)\n",
        "    total_loss_gen = AverageMeter()\n",
        "    total_loss_dis = AverageMeter()\n",
        "\n",
        "    for (HLR, HSR) in loop:\n",
        "        HSR = HSR.to(device)\n",
        "        HLR = HLR.to(device)\n",
        "\n",
        "        HSRhat = gen(HLR)\n",
        "        disc_real = disc(HSR)\n",
        "        disc_fake = disc(HSRhat.detach())\n",
        "        disc_loss_real = mse(disc_real, torch.ones_like(disc_real) - 0.1 * torch.rand_like(disc_real))\n",
        "        disc_loss_fake = mse(disc_fake, torch.zeros_like(disc_fake))\n",
        "        loss_disc = disc_loss_fake + disc_loss_real\n",
        "        total_loss_dis.update(loss_disc)\n",
        "\n",
        "        opt_disc.zero_grad()\n",
        "        loss_disc.backward()\n",
        "        opt_disc.step()\n",
        "\n",
        "        disc_fake = disc(HSRhat)\n",
        "        adversarial_loss = 1e-3 * mse(disc_fake, torch.ones_like(disc_fake))\n",
        "        loss_for_gen = criterion(HSRhat, HSR)\n",
        "        gen_loss = loss_for_gen + adversarial_loss\n",
        "        total_loss_gen.update(gen_loss)\n",
        "\n",
        "        opt_gen.zero_grad()\n",
        "        gen_loss.backward()\n",
        "        opt_gen.step()\n",
        "\n",
        "#################################################### SAVING GENERATOR AND DISCRIMINATOR ################################################\n",
        "    if epoch % 5 == 0 :\n",
        "      path_gen = 'Generator_epoch_{}_{}dB.pth.tar'.format(epoch,snr)\n",
        "      path_disc = 'Discrimnator_epoch_{}_{}dB.pth.tar'.format(epoch,snr)\n",
        "\n",
        "      save_checkpoint(gen, opt_gen, filename=path_gen)\n",
        "      save_checkpoint(disc, opt_disc, filename=path_disc)\n",
        "\n",
        "\n",
        "    discloss.append(total_loss_dis.avg.cpu().detach().numpy())\n",
        "    genloss.append(total_loss_gen.avg.cpu().detach().numpy())\n",
        "\n",
        "    logger.info(f'Train: Epoch:{epoch} \\t Loss_Generator:{total_loss_gen.avg:.4} \\t lr:{lr_scheduler_gen.get_last_lr()}')\n",
        "    logger.info(f'Train: Epoch:{epoch} \\t Loss_Discrimnator:{total_loss_dis.avg:.4} \\t lr:{lr_scheduler_disc.get_last_lr()}')"
      ],
      "metadata": {
        "id": "Nl7vucwsYi5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Valid"
      ],
      "metadata": {
        "id": "rBTkFSidlC2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_fn(val_loader, gen, disc, mse, bce):\n",
        "    gen.eval()\n",
        "    disc.eval()\n",
        "    total_loss_gen = AverageMeter()\n",
        "    total_loss_dis = AverageMeter()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for (x, label) in val_loader:\n",
        "            label = label.to(device)\n",
        "            x = x.to(device)\n",
        "\n",
        "            fake = gen(x)\n",
        "            disc_real = disc(label)\n",
        "            disc_fake = disc(fake)\n",
        "\n",
        "            disc_loss_real = bce(disc_real, torch.ones_like(disc_real) - 0.1 * torch.rand_like(disc_real))\n",
        "            disc_loss_fake = bce(disc_fake, torch.zeros_like(disc_fake))\n",
        "            loss_disc = disc_loss_fake + disc_loss_real\n",
        "            total_loss_dis.update(loss_disc.item())\n",
        "\n",
        "            disc_fake = disc(fake)\n",
        "            adversarial_loss = 1e-3 * bce(disc_fake, torch.ones_like(disc_fake))\n",
        "            loss_for_vgg = mse(fake, label)\n",
        "            gen_loss = loss_for_vgg + adversarial_loss\n",
        "            total_loss_gen.update(gen_loss.item())\n",
        "\n",
        "    gen.train()\n",
        "    disc.train()\n",
        "\n",
        "    avg_val_gen_loss = total_loss_gen.avg\n",
        "    avg_val_dis_loss = total_loss_dis.avg\n",
        "\n",
        "    return avg_val_gen_loss, avg_val_dis_loss\n"
      ],
      "metadata": {
        "id": "lA38sFdslFYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training phase"
      ],
      "metadata": {
        "id": "59EnmASJZkSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load_gen= '/content/Generator_epoch_50.pth.tar'\n",
        "# load_disc='/content/Discrimnator_epoch_50.pth.tar'\n",
        "# load_checkpoint(load_gen,gen,opt_gen,LEARNING_RATE)\n",
        "# load_checkpoint(load_disc, disc, opt_disc,LEARNING_RATE)\n",
        "\n",
        "NMSE_valid = []\n",
        "NMSE_train = []\n",
        "start , end = 1 , 101\n",
        "for epoch in range (start , end):\n",
        "  train_fn(trainloader, disc, gen, opt_gen, opt_disc, mse)\n",
        "  nmse_valid = valid(validloader,gen,1000)\n",
        "  nmse_train = train(trainloader2,gen,4000)\n",
        "  a = nmse_train.sum()/(40000)\n",
        "  b = nmse_valid.sum()/(10000)\n",
        "  NMSE_valid.append(b)\n",
        "  NMSE_train.append(a)\n",
        "  print('NMSE_valid = {}'.format(b))\n",
        "  print('NMSE_train = {}'.format(a))\n",
        "  print('--------------------------------------------------------------')\n",
        "  lr_scheduler_gen.step()\n",
        "  lr_scheduler_disc.step()"
      ],
      "metadata": {
        "id": "ybJcH4uuZm72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "612414b1-72b5-4413-dd85-fd872dd9668d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 3/625 [01:03<3:38:03, 21.03s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-742c32f674ff>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m101\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_disc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m   \u001b[0mnmse_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mnmse_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-aa14c3ff66c1>\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(loader, disc, gen, opt_gen, opt_disc, mse)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mHSRhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHLR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mdisc_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHSR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mdisc_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHSRhat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mdisc_loss_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mdisc_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisc_real\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisc_real\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-fc73f4335451>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-804ed20d1f63>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_act\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.semilogy(NMSE_valid)\n",
        "plt.semilogy(NMSE_train)\n",
        "plt.show()\n",
        "# np.min(NMSE_test)"
      ],
      "metadata": {
        "id": "Hc4WcL9ic1Fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.io import savemat\n",
        "import numpy as np\n",
        "\n",
        "mdic = {\"NMSE_test{}\".format(snr): NMSE_test, \"label\": \"experiment\"}\n",
        "mdic\n",
        "{'a':NMSE_test,'label': 'experiment'}\n",
        "savemat(\"NMSE_test{}.mat\".format(snr), mdic)"
      ],
      "metadata": {
        "id": "qbkIuvubt4wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('generator',genloss[-1])\n",
        "print('discrimnator',discloss[-1])\n",
        "\n",
        "fig2, (generator,semilog_gen) = plt.subplots(1,2, figsize=(15, 2))\n",
        "\n",
        "plt.grid()\n",
        "generator.plot(genloss,label=\"objectloss\",linestyle='-',color='green')\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "print()\n",
        "fig, (discrimnator,semilog_disc) = plt.subplots(1,2, figsize=(15, 2))\n",
        "\n",
        "plt.grid()\n",
        "discrimnator.plot(discloss,label=\"objectloss\",color='blue')\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"Loss\")"
      ],
      "metadata": {
        "id": "PW93UzEDl12a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "tNVFXQjGHRsa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SNR=10.0**(snr/10.0) # transmit power\n",
        "############## training set generation ##################\n",
        "file_dir_test = '/content/test'\n",
        "\n",
        "data_num_test = 1000\n",
        "data_num_file = 1000\n",
        "Y=np.zeros((Nt,M), dtype=float)\n",
        "H_test=np.zeros((data_num_test,Nt,M,ch), dtype=float)\n",
        "H_test_noisy=np.zeros((data_num_test,Nt,M,ch), dtype=float)\n",
        "filedir = os.listdir(file_dir_test)  # type the path of training data\n",
        "\n",
        "# SNR_factor = 11.5\n",
        "SNR_factor = 31.1358\n",
        "SNRr = 0\n",
        "n=0\n",
        "for filename in filedir:\n",
        "    newname = os.path.join(file_dir_test, filename)\n",
        "    data = mat73.loadmat(newname)\n",
        "    # data = sio.loadmat(newname)\n",
        "    channel = data['H']\n",
        "    for i in range(data_num_test):\n",
        "        H=channel[:,:,i]\n",
        "        H_re = np.real(H)\n",
        "        H_im = np.imag(H)\n",
        "        H_ab = np.abs(H)\n",
        "        H_test[i, :, :, 0] = H_re\n",
        "        H_test[i, :, :, 1] = H_im\n",
        "        H_test[i, :, :, 2] = H_ab\n",
        "        phi = np.dot(PHI,np.transpose(np.conjugate(PHI)))\n",
        "        N = np.random.normal(0, 1 / np.sqrt(2), size=(Nt, M)) + 1j * np.random.normal(0, 1 / np.sqrt(2), size=(Nt, M))\n",
        "\n",
        "        N_hat = np.dot(N,phi)\n",
        "        H_hat = np.dot(H,phi)\n",
        "\n",
        "        Y = H_hat + 1.0 / np.sqrt(SNR_factor*SNR) * N_hat\n",
        "        H_hat_re = np.real(Y)\n",
        "        H_hat_im = np.imag(Y)\n",
        "        H_hat_ab = np.abs(Y)\n",
        "        SNRr = SNRr + SNR_factor*SNR * (LA.norm(H_hat)) ** 2 / (LA.norm(N_hat)) ** 2\n",
        "\n",
        "        H_test_noisy[i, :, : ,0] = H_hat_re\n",
        "        H_test_noisy[i, :, :, 1] = H_hat_im\n",
        "        H_test_noisy[i, :, :, 2] = H_hat_ab\n",
        "    n=n+1\n",
        "print(SNRr/(data_num_test))\n",
        "SNRR = 10 * np.log10(SNRr/(data_num_test))\n",
        "print(SNRR)\n",
        "print(H_test.shape,H_test_noisy.shape)"
      ],
      "metadata": {
        "id": "s-dIUeAORyB5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "586f8713-81bd-4428-cb6a-ea5da781f2bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "98.58847683026409\n",
            "19.938261569140032\n",
            "(1000, 16, 144, 3) (1000, 16, 144, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_test=10\n",
        "testset = mydataset(H_test_noisy , H_test)\n",
        "testloader = DataLoader(testset , batch_size = batch_test , shuffle = False, drop_last=True)\n",
        "\n",
        "def test(testloader,model):\n",
        "  nmse=np.zeros((1000,1), dtype=float16)\n",
        "  k = 0\n",
        "  model.eval()\n",
        "  for (input,label) in tqdm(testloader):\n",
        "    nmse2=torch.zeros((batch_test,1), dtype=torch.float16)\n",
        "\n",
        "    input,label = input.to(device),label.to(device)\n",
        "    t1=time.time()\n",
        "    decoded_channel = model(input)\n",
        "    t2=time.time()\n",
        "\n",
        "    for n in range(batch_test):\n",
        "        MSE=((label[n,:,:,:]-decoded_channel[n,:,:,:])**2).sum()\n",
        "        norm_real=((label[n,:,:,:])**2).sum()\n",
        "        nmse2[n]=MSE/norm_real\n",
        "\n",
        "    a = nmse2.sum()\n",
        "    nmse[k] = a.detach().numpy()\n",
        "    k = k + 1\n",
        "\n",
        "  return nmse\n",
        "H_test = []\n",
        "H_test_noisy = []\n",
        "data = []\n",
        "channel = []"
      ],
      "metadata": {
        "id": "rhSDBMo_tIfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Least square & NMSE"
      ],
      "metadata": {
        "id": "JNmN_LeczUcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "load_gen= '/content/Generator_epoch_80_20dB.pth.tar'\n",
        "load_checkpoint(load_gen,gen,opt_gen,LEARNING_RATE)"
      ],
      "metadata": {
        "id": "KbyxLv1iGUCv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e757afc-92ba-4b76-91cc-bae8d006ae04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> Loading checkpoint\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_test=10\n",
        "testset = mydataset(H_test_noisy , H_test)\n",
        "testloader = DataLoader(testset , batch_size = batch_test , shuffle = False, drop_last=True)\n",
        "\n",
        "def test(testloader,model):\n",
        "  nmse=np.zeros((1000,1), dtype=float16)\n",
        "  k = 0\n",
        "  model.eval()\n",
        "  for (input,label) in tqdm(testloader):\n",
        "    nmse2=torch.zeros((batch_test,1), dtype=torch.float16)\n",
        "\n",
        "    input,label = input.to(device),label.to(device)\n",
        "    t1=time.time()\n",
        "    decoded_channel = model(input)\n",
        "    t2=time.time()\n",
        "\n",
        "    for n in range(batch_test):\n",
        "        MSE=((label[n,:,:,:]-decoded_channel[n,:,:,:])**2).sum()\n",
        "        norm_real=((label[n,:,:,:])**2).sum()\n",
        "        nmse2[n]=MSE/norm_real\n",
        "\n",
        "    a = nmse2.sum()\n",
        "    nmse[k] = a.detach().numpy()\n",
        "    k = k + 1\n",
        "\n",
        "  return nmse\n",
        "H_test = []\n",
        "H_test_noisy = []\n",
        "data = []\n",
        "channel = []"
      ],
      "metadata": {
        "id": "P7m8UiXP9d1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nmse = test(testloader,gen)\n",
        "nmse.sum()/(1000)"
      ],
      "metadata": {
        "id": "Ut7_KKoR-Cr1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "191dec27-12a5-4645-ec4c-cdcff6ec85ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 10/10 [00:00<00:00, 55.66it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.04975"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "H_test=(torch.permute(torch.tensor(H_test , dtype = torch.float32),(0,3,1,2))).to(device)\n",
        "H_test_noisy=torch.permute(torch.tensor(H_test_noisy,dtype = torch.float32),(0,3,1,2)).to(device)"
      ],
      "metadata": {
        "id": "SH6C6FZXHj6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "t1=time.time()\n",
        "# dn_model.eval()\n",
        "gen.eval()\n",
        "decoded_channel = gen(H_test_noisy)\n",
        "# decoded_channel = dn_model(H_test_interpolated)\n",
        "t2=time.time()\n",
        "nmse2=torch.zeros((data_num_test,1), dtype=torch.float16)\n",
        "for n in range(data_num_test):\n",
        "    MSE=((H_test[n,:,:,:]-decoded_channel[n,:,:,:])**2).sum()\n",
        "    norm_real=((H_test[n,:,:,:])**2).sum()\n",
        "    nmse2[n]=MSE/norm_real\n",
        "print('NMSE = ',nmse2.sum()/(data_num_test))  # calculate NMSE after training stage (testing performance)\n",
        "\n",
        "print('t2-t1 = ',t2-t1)"
      ],
      "metadata": {
        "id": "yxfUFgvLdazz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6daf079b-6960-4c65-d899-7c1c6f1a3be8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NMSE =  tensor(1.0156, dtype=torch.float16, grad_fn=<DivBackward0>)\n",
            "t2-t1 =  0.011760711669921875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.io import savemat\n",
        "import numpy as np\n",
        "decoded_channel = torch.permute(decoded_channel , (2,3,1,0))\n",
        "decoded_channel = decoded_channel.cpu()\n",
        "x= decoded_channel.detach().numpy()\n",
        "mdic = {\"H_SRGAN_{}\".format(snr): x, \"label\": \"experiment\"}\n",
        "mdic\n",
        "{'a':x,'label': 'experiment'}\n",
        "savemat(\"H_SRGAN_{}.mat\".format(snr), mdic)\n",
        "# !mv '/content/F_RF2.mat' '/content/drive/MyDrive/Colab Notebooks/ML+CE/Dataset/F_RF'"
      ],
      "metadata": {
        "id": "wXunGK8lOqLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# To Save Estimated Channel\n"
      ],
      "metadata": {
        "id": "0gpUqFL6sz6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SNR=10.0**(snr/10.0) # transmit power\n",
        "############## training set generation ##################\n",
        "file_dir_test = '/content/test'\n",
        "\n",
        "data_num_test = 10000\n",
        "data_num_file = 10000\n",
        "Y=np.zeros((Nt,M), dtype=float)\n",
        "H_test=np.zeros((data_num_test,Nt,M,ch), dtype=float)\n",
        "H_test_noisy=np.zeros((data_num_test,Nt,M,ch), dtype=float)\n",
        "filedir = os.listdir(file_dir_test)  # type the path of training data\n",
        "\n",
        "SNR_factor = 31.1358\n",
        "SNRr = 0\n",
        "n=0\n",
        "for filename in filedir:\n",
        "    newname = os.path.join(file_dir_test, filename)\n",
        "    data = mat73.loadmat(newname)\n",
        "    # data = sio.loadmat(newname)\n",
        "    channel = data['H']\n",
        "    for i in range(data_num_test):\n",
        "        H=channel[:,:,i]\n",
        "        H_re = np.real(H)\n",
        "        H_im = np.imag(H)\n",
        "        H_ab = np.abs(H)\n",
        "        H_test[n*data_num_file+i, :, :, 0]=H_re\n",
        "        H_test[n*data_num_file+i, :, :, 1] = H_im\n",
        "        # H_test[n*data_num_file+i, :, :, 2] = H_ab\n",
        "        phi = np.dot(PHI,np.transpose(np.conjugate(PHI)))\n",
        "        N = np.random.normal(0, 1 / np.sqrt(2), size=(Nt, M)) + 1j * np.random.normal(0, 1 / np.sqrt(2), size=(Nt, M))\n",
        "        N_hat = np.dot(N,phi)\n",
        "\n",
        "        H_hat = np.dot(H,phi)\n",
        "        Y = H_hat + 1.0 / np.sqrt(SNR_factor*SNR) * N_hat\n",
        "        H_hat_re = np.real(Y)\n",
        "        H_hat_im = np.imag(Y)\n",
        "        # H_hat_ab = np.abs(Y)\n",
        "        SNRr = SNRr + SNR_factor*SNR * (LA.norm(H_hat)) ** 2 / (LA.norm(N_hat)) ** 2\n",
        "\n",
        "        H_test_noisy[n*data_num_file+i, :, : ,0]=H_hat_re\n",
        "        H_test_noisy[n*data_num_file+i, :, :, 1] = H_hat_im\n",
        "        # H_test_noisy[n*data_num_file+i, :, :, 2] = H_hat_ab\n",
        "    n=n+1\n",
        "print(SNRr/(data_num_test))\n",
        "SNRR = 10 * np.log10(SNRr/(data_num_test))\n",
        "print(SNRR)\n",
        "print(H_test.shape,H_test_noisy.shape)\n",
        "# index3 = np.where(abs(H_test) > 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VTusyc1s7fq",
        "outputId": "9374184f-591e-410e-d1de-1212037fe7bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.1680845891911114\n",
            "5.007967689176081\n",
            "(10000, 16, 144, 2) (10000, 16, 144, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_test=10\n",
        "testset = mydataset(H_test_noisy , H_test)\n",
        "testloader = DataLoader(testset , batch_size = batch_test , shuffle = False, drop_last=True)\n",
        "\n",
        "H_new = []\n",
        "def test(testloader,model):\n",
        "  nmse=np.zeros((1000,1), dtype=float16)\n",
        "  k = 0\n",
        "  model.eval()\n",
        "  for (input,label) in tqdm(testloader):\n",
        "    nmse2=torch.zeros((batch_test,1), dtype=torch.float16)\n",
        "\n",
        "    input,label = input.to(device),label.to(device)\n",
        "    t1=time.time()\n",
        "    decoded_channel = model(input)\n",
        "    t2=time.time()\n",
        "\n",
        "    for n in range(batch_test):\n",
        "        MSE=((label[n,:,:,:]-decoded_channel[n,:,:,:])**2).sum()\n",
        "        norm_real=((label[n,:,:,:])**2).sum()\n",
        "        nmse2[n]=MSE/norm_real\n",
        "\n",
        "    a = nmse2.sum()\n",
        "    nmse[k] = a.detach().numpy()\n",
        "    k = k + 1\n",
        "    ALI = torch.permute(decoded_channel , (0,2,3,1))\n",
        "    H_new.append(ALI.cpu().detach().numpy())\n",
        "    # print('NMSE = ',nmse2.sum()/10)  # calculate NMSE after training stage (testing performance)\n",
        "  return nmse\n",
        "# H_test = []\n",
        "# H_test_noisy = []\n",
        "# data = []\n",
        "# channel = []"
      ],
      "metadata": {
        "id": "h5qh5UA-szcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_gen= '/content/Generator_SNR_-10dB_10.pth.tar'\n",
        "load_checkpoint(load_gen,gen,opt_gen,LEARNING_RATE)"
      ],
      "metadata": {
        "id": "-w-lOBdKtDBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nmse = test(testloader,gen)\n",
        "nmse.sum()/(10000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2XomtkGtFDN",
        "outputId": "261eabf3-68cf-4b40-e6c3-835c72c11913"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 1000/1000 [00:06<00:00, 157.12it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0158625"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "H_SRGAN20 = np.zeros((data_num_test,Nt,M,ch), dtype=float)\n",
        "m = 0\n",
        "for i in range(1000):\n",
        "  a = H_new[i]\n",
        "  for j in range(10):\n",
        "    H_SRGAN20[j+m,:,:,:] = a[j]\n",
        "  m+= 10"
      ],
      "metadata": {
        "id": "eMrfViwG6m2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nmse2=torch.zeros((data_num_test,1), dtype=torch.float32)\n",
        "for n in range(data_num_test):\n",
        "    MSE=((H_test[n,:,:,:]-H_SRGAN20[n,:,:,:])**2).sum()\n",
        "    norm_real=((H_test[n,:,:,:])**2).sum()\n",
        "    nmse2[n]=MSE/norm_real\n",
        "print('NMSE = ',nmse2.sum()/(data_num_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6F1gdh48znL",
        "outputId": "a2761be7-1344-4ca1-fd69-aa68dac3bb86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NMSE =  tensor(0.0159)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "H_SRGAN5 = torch.tensor(H_SRGAN20)\n",
        "H_SRGAN5 = torch.permute(H_SRGAN5 , (1,2,3,0))\n",
        "H_SRGAN5 = H_SRGAN5.cpu().detach().numpy()"
      ],
      "metadata": {
        "id": "-XRFfw5OGSTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.io import savemat\n",
        "import numpy as np\n",
        "\n",
        "mdic = {\"H_SRGAN5\": H_SRGAN5, \"label\": \"experiment\"}\n",
        "mdic\n",
        "{'a':H_SRGAN5,'label': 'experiment'}\n",
        "savemat(\"H_SRGAN5.mat\", mdic)"
      ],
      "metadata": {
        "id": "e4gFcZ-j_dgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv '/content/H_SRGAN5.mat' '/content/drive/MyDrive/RESULT CE'"
      ],
      "metadata": {
        "id": "ihwdG6jW0qui"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}